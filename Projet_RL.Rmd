---
title: "PROJET INFO0806 - RÉGRESSION LINÉAIRE MULTIPLE SUR LE JEU DE DONNÉES << CPUS >>"
author: "réalisé par LIEPO BRICE - KEVIN & ROA SERRANO WALTER"
date: "`r format(Sys.time(), '%d %B, %Y')`"
language: R
cran: http://cran.rstudio.com
output: 
  pdf_document:
    toc: true # table of content true
    toc_depth: 3 # three depths of headings (#, ## and ###)
editor_options: 
  chunk_output_type: inline
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, tidy.opts=list(width.cutoff=60))
library(MASS)
library(knitr)
library(lmtest)
library(car)
library(tidyverse)
```

# **INTRODUCTION**

En statistiques, un modèle de régression linéaire est un modèle de régression qui cherche à établir une relation linéaire entre une variable, dite expliquée, et une ou plusieurs variables, dites explicatives.

La régression linéaire multiple est, quant à elle, une méthode de régression mathématique étendant la régression linéaire simple pour décrire les variations d'une variable endogène (expliquée) associée aux variations de plusieurs variables exogènes (explicatives). Par exemple, une analyse de régression multiple peut révéler une relation positive entre la demande de lunettes de soleil et différents caractères démographiques (âge, salaire) des acheteurs de ce produit. La demande augmente et baisse avec les variations de ces caractéristiques.

Dans le cadre du module info0806, nous sommes ammenés à realiser une régression linéaire sur un jeu de données du nom de "cpus", afin de trouver le modèle de régression adéquat.

# **ENVIRONNMENT DE TRAVAIL**

## **LANGAGE R**
Dans la cadre cet projet nous avons travaillé avec R qui est un langage de programmation open source destiné aux statistiques et à la science des données utilisé largement par les statisticiens, les data miners, data scientists pour l'analyse de données et répresenter ces analyse graphiquement. R dispose d'un très grand nombre de bibliothèques dévéloppées par une communauté de contributeurs qui permettent des ddiverses analyses et d'une grande variété des posibilités pour la répresentation graphique.

## **R STUDIO**
RStudio est un environnement de développement intégre, gratuit, libre et multiplataforme qui permet de travailler en R, sa founction consiste à faciliter le développement d'applications, il dispose de nombreux outils qui vous permettent le traitement de données et l'analyse statistique ainsi comme la création des scripts, la création des graphes et compiler du code.

## **MARKDOWN**
Markdown est une langage de balisage léger, son but est d'offrir une syntaxe facile et qui permet de mélanger du code R avec de text pour générer des documents au format HTML, PDF, Word et d'autres. On va l'utiliser pour l'exportation des résultats d'analyse en PDF.

## **PACKAGES UTILISÉS**

- library(MASS) : Contient des fonctions et jeux de données pour prendre en charge Venables et Ripley. Notre jeu de données "cpus" appartient à cette librairie. 
- library(knitr) : Fournit un outil polyvalent pour la génération de rapports dynamiques en R à l'aide de techniques de programmation littéraire.
- library(lmtest) : Une collection de tests, d'ensembles de données et d'exemples pour la vérification diagnostique dans les modèles de régression linéaire. En outre, certains outils génériques pour l'inférence dans les modèles paramétriques sont fournis.
- library(car) : Utilisée pour appliquer une régression linéaire et des tests statistiques
- library(tidyverse) : C'est une collection d'extensions qui concernent à la data science et qui permet la manipulation des tableaux, la manipulation de variables et la visualisation de données.

# **PRÉSENTATION ET PRÉPARATION DES DONNÉES**

## **PRÉPARATION DES DONNÉES**

Comme annoncé précédemment, nous allons utiliser le jeu de données "cpus", présent dans un des packages R, du nom de MASS. Nous allons effectuer la recupération de ces données et leur mise en forme afin de les utiliser.

```{r}
data(cpus)
cpus <- data.frame(cpus,row.names="name")
attach(cpus)
```


## **PRÉSENTATION DES DONNÉES**

Ce jeu de données resume les performances relatives et les caractéristiques de 209 processeurs. Les differentes variables sont les suivantes :

- syct : temps de cycle en nanosecondes;
- mmin : mémoire principale minimale en kilo-octets;
- mmax : mémoire principale maximale en kilo-octets;
- cach : taille du cache en kilo-octets;
- chmin : nombre minimum de canaux;
- chmax : nombre maximum de canaux;
- perf : performances publiées sur un mix de référence par rapport à un IBM 370 / 158-3;
- estperf : performances estimées (par Ein-Dor & Feldmesser).

Le contenu de ces colonnes se resume comme suit :

### Resume jeu de données 
```{r}
summary(cpus)
```

les dix premiers individus sont les suivants :

### Table des premiers individus 
```{r}
kable(head(cpus,10))
```

Nous allons supprimer la dernière variable de notre jeu de données car, après recherche, nous avons constaté que cette variable represente un critère de prédiction déterminé par le travail de Ein-Dor & Feldmesser.

### Supression de la variable "estperf"

```{r}
cpus$estperf <- NULL
```

## **CHOIX DE VARIABLES**

Pour notre régression linéaire multiple nous avons choisi la variable "perf" comme varible expliquée. En effet nous souhaitons expliquer la performance des processeurs en fonction des autres variables.
Le tableau et le graphe de corrélation suivants nous montreront que, la variable "perf" est la plus corrélée aux autres variables :

### Table des individus et graphe de corrélation 

```{r}
kable(cor(cpus))
plot(cpus)
```

# **RÉGRESSION LINÉAIRE MULTIPLE**

## **APPLICATION DE LA MÉTHODE DESCENDANTE**

Nous allons utiliser une méthode descendante en se servant de la fonction lm(), comme en régression linéaire simple. Dans la fonction lm, le point, placé après le symbole "~", indique qu’on souhaite régresser "perf" sur toutes les autres variables du jeu de données. 

Le critère utilisé par défaut dans R est le critère AIC (pour “An Information Criterion”, proposé par Akaike). La fonction "extractAIC" permet de minimiser ce critère.

La fonction summary() permet de produire les sorties pour notre régression.

### Régression linéaire et AIC avec toutes les variables  
```{r}
summary(perf.lm <- lm(formula=perf~.,data=cpus))
extractAIC(perf.lm)
```

Les informations données par la fonction summary() concernent :

- Les résidus (maximum, minimum, moyenne, quartiles);
- Les coefficients estimés : Les estimations (Estimate), l’écart-type estimé des estimateurs correspondants (Std. Error), la valeur de la statistique de test (t value) et la p-value (Pr(>|t|)) associées aux tests (probabilité que le coefficient soit significativement différent de zéro);
- La qualité d’adéquation du modèle : une estimation de l’écart-type du terme d’erreur (Residual standard error), la valeur du R2 (Multiple R-squared) et celle du R2 ajusté (Adjusted R-squared), et enfin la valeur de la statistique de test (F-statistic : testant la significativité globales des variables), son degré de liberté et la p-value du test de Fisher de significativité du modèle.

On soustrait à présent la variable "chmin" car son coefficient n'est pas significativement différent de zéro et sa p-valeur est supérieur à 0.3.

### Régression linéaire et AIC sans la variable "chmin" 
```{r}
summary(perf.lm <- update(perf.lm,.~.-chmin))
extractAIC(perf.lm)
```

Nous constatons que l’AIC et l'ecart type ont légerement diminué. Ce qui signifie que ce modèle est plus efficace que le précédent. Nous conservons donc ce model pour l'instant.

Nous allons retirer ensuite la variable dont le coefficient est le moins significatif, à savoir la variable "syct".

### Régression linéaire et AIC sans la variable "syct" 
```{r}
summary(perf.lm<-update(perf.lm,.~.-syct))
extractAIC(perf.lm)
```

Nous constatons que l’AIC a augmenté et l’écart-type estimé des résidus est passé de 59.86 à 60.86. e qui signifie que ce modèle est moins efficace que le précédent. Nous allons par conséquent conserver le modèle contenant la variable "syct" et afficher par la meme occasion les coefficiant de la droite.

### Resume et Coefficient du model calibré 
```{r}
summary(perf.lm<-lm(formula = perf ~ syct + mmin + mmax + cach + chmax, data = cpus))
coef(perf.lm)
```

## **ÉTUDE DES RÉSIDUS**

Nous allons recupérer et afficher sous forme d'histograme et de graphes l'ensemble des résidus, afin d'observer si les conditions d'indépendance, d'homoscédasticité (même variance) et de normalité sont respectés. Mais aussi vérifier que ces residus peuvent suivre la loi normale. 

Les représentations sont, respectivement, les suivantes :

### Histogrammes des résidus 

- Un histogramme des résidus;
- Un histogramme des résidus avec une courbe pour vérifier si ces residus peuvent suivre la loi normale;

```{r}
par(mfrow=c(1,2))
hist(perf.lm$residuals,freq=FALSE,nclass=10, col="lightblue",main="histogramme des résidus")
histo <- hist(perf.lm$residuals, col="lightblue", main="histogramme des résidus", probability = TRUE)
ec_typ <- summary(perf.lm)$sigma
curve(dnorm(x, 0, ec_typ), from = min(histo$breaks), to = max(histo$breaks), 
    add = TRUE, type = "l", col = "magenta", lwd = 2)

```

### Graphes de répartition des résidus

- La répartition des residus;
- La répartition des residus par rapport à la mémoire principale minimale en kilo-octets (mmin);
- La répartition des residus par rapport au nombre maximum de canaux (chmax);
- La répartition des residus par rapport aux valeurs prédites (fitted value);

```{r}
par(mfrow=c(2,2))
plot(perf.lm$residuals,main="Résidus")
abline(h=0,col="magenta")

plot(cpus$mmin,perf.lm$residuals,main="Résidus")
abline(h=0,col="magenta")

plot(cpus$chmax,perf.lm$residuals,main="Résidus")
abline(h=0,col="magenta")

plot(perf.lm$fitted.value,perf.lm$residuals,main="Résidus")
abline(h=0,col="magenta")
```

Nous constatons dans un premier temps, grace aux deux premier histogramme, que les résidus suivent bel et bien la loi normale.

Nous remarquons aussi que les résidus sont repartis, de manière relativement indépendantes, de part et d'autre de la droite d'équation y=0. 

Si les résidus n'était pas répartis de manière relativement indépendantes, de part-et d'autre de la droite d'équation y=0, il y aurait eu un problème (corrélation(s), hétéroscédasticité, ...) que l'on allait devoir corriger avant de pouvoir interpréter un résultat.

De plus, nous remarquons une diminution des résidus (négatifs) lorsque la mémoire principale minimale en kilo-octets (mmin), le nombre maximum de canaux (chmax), ou la valeur prédite augmentent.

Pour finir nous allons afficher :

- La repartition des valeurs prédites (en ordonnée) et des valeurs de "perf" (en abscisse);
- La repartition des quantiles.

### Graphes des valeurs prédites et des quantiles 
```{r}
par(mfrow=c(1,2))

plot(cpus$perf, perf.lm$fitted.values)
abline(0, 1, col = "magenta", lwd = 2)

ec_typ <- summary(perf.lm)$sigma
normed_res <- perf.lm$residuals/ec_typ
qqnorm(normed_res, xlim = range(normed_res), ylim = range(normed_res))
abline(0, 1, col = "magenta", lwd = 2)
```


### Graphe de répresentation de l'independence ou l'autocorrélation des résidus  
```{r}
acf(perf.lm$residuals,ci=0.99)
```

## **PRÉDICTION**

Nous allons afficher les valeurs prédites par rapport aux valeurs normales: 

### Graph des valeurs prédites 
```{r}
plot(perf.lm$fitted.values, cpus$perf)
```

Partons du principe que nous avons un nouvel individu. L’intervalle de confiance sur la valeur prédite est donné par l’instruction suivante :

### Intervale de confiance sur la valeur prédite
```{r}
predict(perf.lm,data.frame(syct=125, mmin=256, mmax=6000, cach=256, chmax=128),interval="prediction",level=0.95)
```

Par conséquent, si la performance (perf) est comprise dans l’intervalle [191.76; 466.11], il n’y a pas de contradiction avec le modèle.

# **TESTS STATISTIQUES**

Nous allons réaliser des tests statistiques afin de valider les hypothèses :

La p-value est souvent utilisée dans les tests d'hypothèses, ce test nous permet de rejeter, ou non, une hypothèse nulle. Elle représente la probabilité de faire une erreur de type 1 ou « faux positif », ou de rejeter l'hypothèse nulle si elle est vrai.

Plus la valeur p-value est petite, plus la probabilité de faire une erreur en rejetant l'hypothèse nulle est faible. Une valeur limite de 0,05 est souvent utilisée. Autrement dit, vous pouvez rejeter l'hypothèse nulle si la p-valeur est inférieure à 0,05.

## **TEST D'ADÉQUATION OU DE LINÉARITÉ**

Pour conclure à la non-linéarité du modèle de régression, on préconise le test de Rainbow : si p-valeur < 0.05, on rejette la linéarité du modèle.
```{r}
raintest(perf.lm)
```
 
On a obtenu une p-value < 2.2e-16 donc on peut rejeter l'hypothèse nulle.

Le résultat donne une p-valeur < 0.05 donc, nous pouvons en conclure qu'un modèle de régression non-linéaire est plus adapté aux données.

## **TEST DE NORMALITÉ DE SHAPIRO**

En statistique, le test de Shapiro–Wilk teste l'hypothèse nulle selon laquelle un échantillon est issu d'une population normalement distribuée. En d'autre terme, le test de Shapiro-Wilk permet de savoir si une série de données suit une loi normale. Il a été publié en 1965 par Samuel Sanford Shapiro et Martin.
```{r}
shapiro.test(perf.lm$residuals)
```

Ci-dessus, la p-value est significative, donc les résidus ne suivent pas une loi normale.

## **TEST D’HOMOSCÉDASTICITÉ DES VARIANCES DE GOLDFELD-QUANDT**

Le test de Goldfeld et Quandt (formulé en 1965) est un test statistique, très utilisé en économétrie dans le cadre d'un modèle linéaire multiple estimé par la méthode des moindres carrés afin de savoir si les perturbations sont hétéroscédastiques ou homoscédastiques. Ce test s'appuie sur la loi de Fisher.
```{r}
gqtest(perf.lm)
```

L'application du test de Goldfeld-Quandt permet de vérifier l'homogénéité des variances des résidus.

Les résultats donnent une p-value > 0,05 par conséquent l'homogénéité des résidus est respectée.

## **TESTS D'INDÉPENDANCE ET D'AUTOCORELATION DES RÉSIDUS DE DURBIN WATSON**

Le test de Durbin-Watson est un test statistique destiné à tester l'autocorrélation des résidus dans un modèle de régression linéaire. Il a été proposé en 1950 et 1951 par James Durbin et Geoffrey Watson.

Il permet de détecter une auto-corrélation des erreurs d'ordre un et il repose sur l'estimation d'un modèle autorégressif de premier ordre pour les résidus estimés.
```{r}
dwtest(perf.lm)
```

Dans l'application du test de Durbin-Watson si la p-value est > 0,05 l'independance des résidus est respectée. Nos résultats donnent une p-value de 1.126e-09 qui est beaucoup plus inférieur à 0,05 Par conséquent l'indépendance des résidus est rejetée.

Nous constatons enfin qu'il y a colinéarité forte dans le modèle, c'est-à-dire que les variables explicatives sont linéairement dépendantes.

## **TEST DE COLINÉARITÉ FORTE DU MODELE** 

Dans une régression, la multicolinéarité est un problème qui survient lorsque certaines variables de prévision du modèle mesurent le même phénomène.

One dit que des variables sont s'il existe une relation linéaire entre elles. Une erreur fréquente est de confondre multicolinéarité et corrélation. Si des variables colinéaires sont de facto fortement corrélées entre elles, deux variables corrélées ne sont pas forcément colinéaires. En termes non statistiques, il y a colinéarité lorsque deux ou plusieurs variables mesurent la même chose.

Pour étudier la colinéarité des variables explicatives l’approche la plus classique consiste à examiner les facteurs d’inflation de la variance (FIV) ou variance inflation factor (VIF). La fonction vif() de la librairie car permet de comparer la colinéarité des variables. Les valeurs renvoyées par vif doivent être bas sinon les variables ont tendance à évoluer dans la même direction.
```{r}
vif(perf.lm)
```

Dans nos résultats, les valeurs des VIF (Variance Inflation Factors) des variables explicatives sont inférieures à 10 par conséquent les variables ne présentent pas de forte colinéarité.

# **CONCLUSION**

Pour conclure, ce projet, réalisé dans le cadre du module info0806, qui avait pour objectif de realiser une régression linéaire sur un jeu de données de notre choix nous a permis de :

- Determiner l'impact des variables explicatives sur les prédictions de la variable expliquée;
- Appliquer les differentes fonctions relatives à la réalisation d'une régression linéaire multiple;
- Comprendre les differents paramètres associés au resultat des differentes fonctions.

